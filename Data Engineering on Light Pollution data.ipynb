{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb9d8722-e385-4b40-a03c-5e1ea4aad372",
   "metadata": {},
   "source": [
    "# Data Engineering on Light Pollution data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b23e151-2e47-4805-8c96-415e7c7b6558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fb3b19-3381-4da8-9d94-1d324ac4465e",
   "metadata": {},
   "source": [
    "# Task: Use the file of URLs from the first assignment to load the data from 2014-2024 and store it all into a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "818c47a8-b39e-49b8-b9da-39e82f074f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load urls file that is text file which was created in Assignment 1\n",
    "with open(\"data/gan_urls.txt\", \"r\") as f:\n",
    "    urls = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "818951b0-1540-46ad-80e9-bdd71164fe42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading http://globeatnight.org/documents/926/GaN2024.csv...\n",
      "Loading http://globeatnight.org/documents/661/GaN2023.csv...\n",
      "Loading http://globeatnight.org/documents/662/GaN2022.csv...\n",
      "Loading http://globeatnight.org/documents/663/GaN2021.csv...\n",
      "Loading http://globeatnight.org/documents/679/GaN2020.csv...\n",
      "Loading http://globeatnight.org/documents/665/GaN2019.csv...\n",
      "Loading http://globeatnight.org/documents/666/GaN2018.csv...\n",
      "Loading http://globeatnight.org/documents/667/GaN2017.csv...\n",
      "Loading http://globeatnight.org/documents/668/GaN2016.csv...\n",
      "Loading http://globeatnight.org/documents/669/GaN2015.csv...\n",
      "Loading http://globeatnight.org/documents/670/GaN2014.csv...\n",
      "Loading http://globeatnight.org/documents/671/GaN2013.csv...\n",
      "Loading http://globeatnight.org/documents/672/GaN2012.csv...\n",
      "Loading http://globeatnight.org/documents/673/GaN2011.csv...\n",
      "Loading http://globeatnight.org/documents/674/GaN2010.csv...\n",
      "Loading http://globeatnight.org/documents/675/GaN2009.csv...\n",
      "Loading http://globeatnight.org/documents/676/GaN2008.csv...\n",
      "Loading http://globeatnight.org/documents/677/GaN2007.csv...\n",
      "Loading http://globeatnight.org/documents/678/GaN2006.csv...\n"
     ]
    }
   ],
   "source": [
    "# Load all CSVs and combine it into single DataFrame\n",
    "dataframes = []\n",
    "for url in urls:\n",
    "    print(f\"Loading {url}...\")\n",
    "    df = pd.read_csv(url, low_memory=False)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all into one DataFrame\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Remove duplicates and reset the index\n",
    "df = df.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed56f374-73a3-4302-a9ad-575de0e7abd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'ObsType', 'Latitude', 'Longitude', 'Elevation(m)', 'LocalDate', 'LocalTime', 'UTDate', 'UTTime', 'LimitingMag', 'SQMReading', 'SQMSerial', 'CloudCover', 'Constellation', 'SkyComment', 'LocationComment', 'Country']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5ec3093-22d3-4eb9-a513-e435fba00f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"year\"] = pd.to_datetime(df[\"LocalDate\"]).dt.year\n",
    "df = df[(df[\"year\"] >= 2014) & (df[\"year\"] <= 2024)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca148adf-c636-4576-abfa-39d43e1a36ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014 2024\n"
     ]
    }
   ],
   "source": [
    "print(df[\"year\"].min(), df[\"year\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075d5ed0-b61d-464c-9918-c186ab45c608",
   "metadata": {},
   "source": [
    "# Task: Merge LocalDate and LocalTime and convert to a datetime object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76dfbe85-a992-4d72-9f4f-e323c2f9a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LocalDate and LocalTime into LocalDateTime\n",
    "\n",
    "df[\"LocalDateTime\"] = pd.to_datetime(df[\"LocalDate\"] + \" \" + df[\"LocalTime\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263cb294-27fe-430d-b0df-36980be056d0",
   "metadata": {},
   "source": [
    "# Task: Convert categorical fields to numeric ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d3fc84b-6b3b-4868-95e3-47461a96d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert CloudCover to numeric percentages\n",
    "\n",
    "cloud_map = {\n",
    "    \"clear\": 0,\n",
    "    \"1/4 of sky\": 0.25,\n",
    "    \"1/2 of sky\": 0.50,\n",
    "    \"over 1/2 of sky\": 0.75\n",
    "}\n",
    "df[\"CloudCoverPct\"] = df[\"CloudCover\"].map(cloud_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76181a8-4c8a-4114-ab15-2d0b0ad50a0a",
   "metadata": {},
   "source": [
    "# Task: Convert Constellation to numeric binary values (binarize the column).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33cf7f57-5746-4278-8152-fdce3804c39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 15 Constellation values to columns which contain a binary value\n",
    "\n",
    "const_dummies = pd.get_dummies(df[\"Constellation\"], prefix=\"const\")\n",
    "df = pd.concat([df, const_dummies], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90a2948-76b6-48c7-8c66-f659bd50c9cc",
   "metadata": {},
   "source": [
    "# Task: Create 5 binary columns loc_urban, loc_suburban, loc_rural, loc_remote and loc_unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fbc5be3-3b45-45dd-9601-71c429a1d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process LocationComment into loc_* columns\n",
    "\n",
    "def assign_location(comment: str) -> str:\n",
    "    if not isinstance(comment, str):\n",
    "        return \"unknown\"\n",
    "    comment = comment.lower()\n",
    "    if any(word in comment for word in [\"city\", \"downtown\", \"urban\", \"metro\", \"metropolitan\"]):\n",
    "        return \"urban\"\n",
    "    elif any(word in comment for word in [\"suburb\", \"suburban\", \"residential\", \"town\"]):\n",
    "        return \"suburban\"\n",
    "    elif any(word in comment for word in [\"farm\", \"village\", \"countryside\", \"rural\", \"small town\"]):\n",
    "        return \"rural\"\n",
    "    elif any(word in comment for word in [\"remote\", \"mountain\", \"desert\", \"forest\", \"isolated\"]):\n",
    "        return \"remote\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "# (a) convert all LocationComment to a single string using assign_location(),\n",
    "# (b) use fillna(\"unknown\") to use the string \"unknown\" as a placeholder.\n",
    "df[\"LocationComment\"] = df[\"LocationComment\"].apply(assign_location).fillna(\"unknown\")\n",
    "\n",
    "# c) use get_dummies() to make the values binary, note: you must prefix the new columns with loc.\n",
    "loc_dummies = pd.get_dummies(df[\"LocationComment\"], prefix=\"loc\")\n",
    "\n",
    "# (d) merge the new columns from (c) in the DataFrame using concat().\n",
    "df = pd.concat([df, loc_dummies], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bfa809-aa1f-4c3f-9ec5-fb29708c34df",
   "metadata": {},
   "source": [
    "# Task: Convert missing SQMReading values to -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc04e1e0-a83d-4f8a-a6de-72207a1a86f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing SQMReading with -1 ( Instead of removing these rows, we will fill them with an invalid value of -1.)\n",
    "\n",
    "df[\"SQMReading\"] = df[\"SQMReading\"].fillna(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5064dd69-ac85-417a-876f-f9b9049b1caa",
   "metadata": {},
   "source": [
    "# Task: Store a new dataset file with a reduced set columns which include our numeric ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63d7fa64-7d57-4b3b-8707-f58d214ad026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select final 27 columns\n",
    "\n",
    "cols_to_keep = [\n",
    "    'Latitude','Longitude','Elevation(m)','LocalDateTime','LimitingMag','SQMReading','CloudCoverPct',\n",
    "    'const_Bootes','const_Canis Major','const_Crux','const_Cygnus','const_Gemini','const_Grus','const_Hercules',\n",
    "    'const_Leo','const_None','const_Orion','const_Pegasus','const_Perseus','const_Sagittarius','const_Scorpius',\n",
    "    'const_Taurus','loc_remote','loc_rural','loc_suburban','loc_unknown','loc_urban'\n",
    "]\n",
    "\n",
    "# Create any missing const_* or loc_* columns with default 0\n",
    "for col in cols_to_keep:\n",
    "    if col not in df.columns:\n",
    "        df[col] = 0\n",
    "\n",
    "# Keep only desired columns\n",
    "df_working = df[cols_to_keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2418d8d-8217-4b35-849d-d2e130a06d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full working dataset saved as '2014_to_2024_gan_data_working.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save final working dataset\n",
    "\n",
    "df_working.to_csv(\"2014_to_2024_gan_data_working.csv\", index=False)\n",
    "print(\"Full working dataset saved as '2014_to_2024_gan_data_working.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74e05cc3-ed63-416e-a96f-b20e6e76b99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS: Normalize column names in working dataset.\n",
    "df_final = df_working.copy()\n",
    "df_final.columns = (\n",
    "    df_final.columns\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace(\" \", \"_\")\n",
    "    .str.replace(\"(\", \"\")\n",
    "    .str.replace(\")\", \"\")\n",
    ")\n",
    "\n",
    "# Save the final dataset\n",
    "df_final.to_csv(\"2014_to_2024_gan_data_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4c564a-29e6-4547-b7a3-6b9a62be7006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
